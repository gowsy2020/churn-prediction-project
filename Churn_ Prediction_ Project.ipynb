{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e595242a-6ec6-42b6-b559-b394b20196e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Task 1: Clean & preprocess the telco dataset, engineer features, and define a robust cross-validation strategy.\n",
    "pip install shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# 1) Create a synthetic telco-style dataset (structure matches typical churn dataset)\n",
    "def make_synthetic_telco(n=7043, random_state=42):\n",
    "    rng = np.random.RandomState(random_state)\n",
    "    tenure = rng.randint(0, 72, size=n)\n",
    "    monthly = np.round(rng.uniform(18, 130, size=n), 2)\n",
    "    total = np.round(np.maximum(tenure * monthly + rng.normal(0, 50, size=n), 0), 2)\n",
    "    senior = rng.choice([0,1], size=n, p=[0.85,0.15])\n",
    "    phone = rng.choice([0,1], size=n, p=[0.12,0.88])\n",
    "    internet = rng.choice(['DSL','Fiber','No'], size=n, p=[0.4,0.4,0.2])\n",
    "    contract = rng.choice(['Month-to-month','One year','Two year'], size=n, p=[0.55,0.25,0.2])\n",
    "    tech = rng.choice([0,1], size=n, p=[0.6,0.4])\n",
    "    # generate churn probability: higher for short tenure, fiber, month-to-month, high charges, no tech support\n",
    "    prob = (\n",
    "        0.35*(tenure<12).astype(float) +\n",
    "        0.25*(internet=='Fiber').astype(float) +\n",
    "        0.2*(contract=='Month-to-month').astype(float) +\n",
    "        0.0007 * (monthly - monthly.mean()) +\n",
    "        -0.15 * tech\n",
    "    )\n",
    "    prob = 1/(1+np.exp(- (prob - prob.mean())))  # squash\n",
    "    churn = (rng.rand(n) < prob).astype(int)\n",
    "    df = pd.DataFrame({\n",
    "        'customerID': [f\"C{100000+i}\" for i in range(n)],\n",
    "        'tenure': tenure,\n",
    "        'MonthlyCharges': monthly,\n",
    "        'TotalCharges': total,\n",
    "        'SeniorCitizen': senior,\n",
    "        'PhoneService': phone,\n",
    "        'InternetService': internet,\n",
    "        'Contract': contract,\n",
    "        'TechSupport': tech,\n",
    "        'Churn': churn\n",
    "    })\n",
    "    return df\n",
    "\n",
    "df = make_synthetic_telco()\n",
    "print(\"Dataset shape:\", df.shape)\n",
    "df.head()\n",
    "\n",
    "# 2) Basic cleaning & feature engineering\n",
    "# - Fill/verify TotalCharges\n",
    "df['TotalCharges'] = df['TotalCharges'].fillna(df['tenure'] * df['MonthlyCharges'])\n",
    "\n",
    "# - Feature engineering: tenure_buckets, monthly_flag (high_spender), encode categoricals\n",
    "df['tenure_bucket'] = pd.cut(df['tenure'],\n",
    "                             bins=[-1,3,12,24,48,72],\n",
    "                             labels=['0-3','4-12','13-24','25-48','49+'])\n",
    "df['high_monthly'] = (df['MonthlyCharges'] > df['MonthlyCharges'].median()).astype(int)\n",
    "\n",
    "# One-hot encode categoricals\n",
    "cat_cols = ['InternetService','Contract','tenure_bucket']\n",
    "df_enc = pd.get_dummies(df.drop(columns=['customerID']), columns=cat_cols, drop_first=True)\n",
    "\n",
    "# 3) Train-test split (holdout) + stratification on Churn\n",
    "X = df_enc.drop(columns=['Churn'])\n",
    "y = df_enc['Churn']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=42)\n",
    "\n",
    "# 4) Scaling numeric columns\n",
    "num_cols = ['tenure','MonthlyCharges','TotalCharges']\n",
    "scaler = StandardScaler()\n",
    "X_train[num_cols] = scaler.fit_transform(X_train[num_cols])\n",
    "X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "# 5) Cross-validation strategy: Stratified K-Fold (5 folds)\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "print(\"Preprocessing complete. X_train shape:\", X_train.shape)\n",
    "#===========================================================================================\n",
    "#TASK 2: Train and tune models and compute metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Helper function to evaluate model on test set\n",
    "def evaluate_model(name, model, X_test, y_test):\n",
    "    probs = model.predict_proba(X_test)[:,1]\n",
    "    preds = model.predict(X_test)\n",
    "    roc_auc = roc_auc_score(y_test, probs)\n",
    "    ap = average_precision_score(y_test, probs)  # AUC-PR (average precision)\n",
    "    p, r, f, _ = precision_recall_fscore_support(y_test, preds, average='binary', zero_division=0)\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    return dict(name=name, roc_auc=roc_auc, avg_precision=ap, precision=p, recall=r, f1=f, confusion_matrix=cm)\n",
    "\n",
    "results_list = []\n",
    "\n",
    "# 2A: Logistic Regression baseline\n",
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train, y_train)\n",
    "results_list.append(evaluate_model(\"LogisticRegression\", lr, X_test, y_test))\n",
    "\n",
    "# 2B: XGBoost (if available)\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    xgb_clf = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)\n",
    "    # small randomized search for core hyperparams\n",
    "    param_dist = {\n",
    "        'n_estimators': [100,200],\n",
    "        'max_depth': [3,5,7],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'subsample': [0.6,0.8,1.0]\n",
    "    }\n",
    "    rsearch = RandomizedSearchCV(xgb_clf, param_distributions=param_dist, n_iter=6, cv=cv, scoring='roc_auc', n_jobs=-1, random_state=42)\n",
    "    rsearch.fit(X_train, y_train)\n",
    "    best_xgb = rsearch.best_estimator_\n",
    "    results_list.append(evaluate_model(\"XGBoost\", best_xgb, X_test, y_test))\n",
    "    print(\"XGBoost best params:\", rsearch.best_params_)\n",
    "except Exception as e:\n",
    "    print(\"XGBoost not available, falling back to RandomForest for advanced model. Error:\", e)\n",
    "    rf = RandomForestClassifier(n_estimators=200, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    results_list.append(evaluate_model(\"RandomForest\", rf, X_test, y_test))\n",
    "    best_xgb = rf  # fallback reference\n",
    "\n",
    "# 2C: LightGBM (if available)\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    lgb_clf = lgb.LGBMClassifier(random_state=42)\n",
    "    param_dist_lgb = {\n",
    "        'n_estimators': [100,200],\n",
    "        'num_leaves': [31,50,80],\n",
    "        'learning_rate': [0.01,0.05,0.1],\n",
    "        'subsample': [0.6,0.8,1.0]\n",
    "    }\n",
    "    rsearch_lgb = RandomizedSearchCV(lgb_clf, param_distributions=param_dist_lgb, n_iter=6, cv=cv, scoring='roc_auc', n_jobs=-1, random_state=42)\n",
    "    rsearch_lgb.fit(X_train, y_train)\n",
    "    best_lgb = rsearch_lgb.best_estimator_\n",
    "    results_list.append(evaluate_model(\"LightGBM\", best_lgb, X_test, y_test))\n",
    "    print(\"LightGBM best params:\", rsearch_lgb.best_params_)\n",
    "except Exception as e:\n",
    "    print(\"LightGBM not available, skipping. Error:\", e)\n",
    "\n",
    "# 2D: Collect results and print table\n",
    "import pandas as pd\n",
    "metrics_df = pd.DataFrame(results_list)\n",
    "metrics_df = metrics_df[['name','roc_auc','avg_precision','precision','recall','f1']]\n",
    "print(metrics_df.to_string(index=False))\n",
    "#===========================================================================================================================\n",
    "#TASK 3: SHAP global analysis\n",
    "try:\n",
    "    import shap\n",
    "    # Use the selected best model (prefer XGBoost > LightGBM > RandomForest)\n",
    "    final_model = None\n",
    "    for candidate in ['XGBoost','LightGBM','RandomForest','LogisticRegression']:\n",
    "        if candidate in [r['name'] for r in results_list]:\n",
    "            # find the corresponding trained estimator\n",
    "            for r in results_list:\n",
    "                if r['name'] == candidate:\n",
    "                    # We must pick the trained estimator variable from above. The code above\n",
    "                    # assigned best_xgb, best_lgb, rf or lr accordingly. To be safe, reconstruct:\n",
    "                    pass\n",
    "    # For clarity, we assume best_xgb or best_lgb or rf (as named when training) exists in scope:\n",
    "    try:\n",
    "        model_for_shap = best_xgb\n",
    "    except NameError:\n",
    "        try:\n",
    "            model_for_shap = best_lgb\n",
    "        except NameError:\n",
    "            model_for_shap = rf  # fallback\n",
    "    # Create TreeExplainer for tree models\n",
    "    explainer = shap.TreeExplainer(model_for_shap)\n",
    "    shap_values = explainer.shap_values(X_train)  # use training or test set\n",
    "    # Global summary (produce plots when running interactively)\n",
    "    shap.summary_plot(shap_values, X_train, show=True)\n",
    "    shap.summary_plot(shap_values, X_train, plot_type=\"bar\", show=True)\n",
    "except Exception as e:\n",
    "    print(\"SHAP not available or failed: \", e)\n",
    "    print(\"Install shap: pip install shap  (then re-run this cell)\")\n",
    "# ============================================================\n",
    "# TASK 4 â€” Local SHAP Explanations (fixed + robust version)\n",
    "import shap\n",
    "import numpy as np\n",
    "\n",
    "# 1. Make sure SHAP is installed\n",
    "try:\n",
    "    shap.__version__\n",
    "except:\n",
    "    raise ImportError(\"Run: pip install shap\")\n",
    "\n",
    "# 2. Ensure LightGBM model is used\n",
    "final_model = best_lgb   # replace with your lightgbm variable name\n",
    "\n",
    "# 3. Recompute SHAP explanation safely\n",
    "explainer = shap.TreeExplainer(final_model)\n",
    "raw_shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# ---- FIX: Handle binary output (1 array) or multiclass (list of arrays) ----\n",
    "if isinstance(raw_shap_values, list):\n",
    "    shap_values = raw_shap_values[1]   # Class 1 = churn\n",
    "else:\n",
    "    shap_values = raw_shap_values      # Already correct (binary)\n",
    "\n",
    "# 4. Use your selected indices\n",
    "indices = {\n",
    "    \"predicted_churn\": 1058,\n",
    "    \"predicted_not_churn\": 1056,\n",
    "    \"borderline\": 18\n",
    "}\n",
    "\n",
    "# 5. Generate SAFE Local Plots + Text Outputs\n",
    "local_outputs = {}\n",
    "\n",
    "for label, idx in indices.items():\n",
    "    print(f\"\\n===== Local SHAP Explanation for: {label} (Index {idx}) =====\")\n",
    "\n",
    "    # Raw contribution values\n",
    "    vals = shap_values[idx]\n",
    "    feature_contribs = sorted(\n",
    "        list(zip(X_test.columns, vals)),\n",
    "        key=lambda x: abs(x[1]),\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    # Save text explanation\n",
    "    local_outputs[label] = feature_contribs[:10]\n",
    "\n",
    "    print(\"\\nTop 10 Feature Contributions:\")\n",
    "    for f, v in feature_contribs[:10]:\n",
    "        print(f\"{f:25s}  SHAP = {v:.4f}\")\n",
    "\n",
    "    # Plot waterfall\n",
    "    try:\n",
    "        shap.plots.waterfall(\n",
    "            shap.Explanation(\n",
    "                values=vals,\n",
    "                base_values=explainer.expected_value,\n",
    "                data=X_test.iloc[idx],\n",
    "                feature_names=X_test.columns\n",
    "            ),\n",
    "            max_display=10\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(\"Plot failed:\", e)\n",
    "\n",
    "local_outputs\n",
    "#====================================================================================================\n",
    "#Task5: Synthesize global and local SHAP findings into a concise strategic recommendations document for the retention team.\n",
    "'''Prioritize converting month-to-month customers into 1-2 year contracts. (Strongest global SHAP churn driver.)\n",
    "\n",
    "Implement proactive onboarding for customers with tenure < 3 months. (High SHAP influence from low tenure.)\n",
    "\n",
    "Provide pricing relief or bundles for high MonthlyCharges customers. (MonthlyCharges consistently contributes positively to churn.)\n",
    "\n",
    "Strengthen support for Fiber customers. (Fiber shows higher SHAP churn contribution.)\n",
    "\n",
    "Deploy TechSupport-based retention initiatives. (Lack of TechSupport pushes churn in SHAP.)\n",
    "\n",
    "Use local SHAP for personalized retention messaging:\n",
    "\n",
    "Show customers exactly which issues affect their risk.\n",
    "Tailor offers to the specific local SHAP drivers.'''\n",
    "#=========================================================================\n",
    "def textual_shap_local(idx, shap_values, X_test, model):\n",
    "    \"\"\"\n",
    "    Generate clean textual SHAP explanations for a single sample.\n",
    "    idx      = index in X_test\n",
    "    shap_values = array of SHAP values for class 1\n",
    "    model    = trained LightGBM or RandomForest model\n",
    "    \"\"\"\n",
    "\n",
    "    row_vals = shap_values[idx]\n",
    "    row = X_test.iloc[idx]\n",
    "\n",
    "    feature_importance = sorted(\n",
    "        zip(X_test.columns, row_vals),\n",
    "        key=lambda x: abs(x[1]),\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    top_pos = [(f, v) for f, v in feature_importance if v > 0][:6]\n",
    "    top_neg = [(f, v) for f, v in feature_importance if v < 0][:6]\n",
    "\n",
    "    prob = model.predict_proba([row])[0,1]\n",
    "\n",
    "    lines = []\n",
    "    lines.append(f\"Index: {idx}, Predicted probability: {prob:.3f}\")\n",
    "    lines.append(\"Top positive SHAP contributors:\")\n",
    "    for f, v in top_pos:\n",
    "        lines.append(f\"  + {f}: SHAP {v:.4f}\")\n",
    "\n",
    "    lines.append(\"Top negative SHAP contributors:\")\n",
    "    for f, v in top_neg:\n",
    "        lines.append(f\"  - {f}: SHAP {v:.4f}\")\n",
    "\n",
    "    return \"\\n\".join(lines)\n",
    "#=========================================================================\n",
    "print(textual_shap_local(1058, shap_values, X_test, final_model))\n",
    "print()\n",
    "print(textual_shap_local(1056, shap_values, X_test, final_model))\n",
    "print()\n",
    "print(textual_shap_local(18, shap_values, X_test, final_model))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
